{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train EpiDeNet (single channel)\n",
    "In this script, the single-channel version of EpiDeNet is trained.\n",
    "\n",
    "We first define the different models to train and test."
   ],
   "id": "7db0f4944c14462e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T15:00:14.215925Z",
     "start_time": "2024-09-02T15:00:14.013445Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_WINDOW_SIZE = 1016\n",
    "INPUT_NB_CHANNELS = 1\n",
    "\n",
    "# Original one-channel epidenet\n",
    "epidenet_1ch_og = tf.keras.Sequential()\n",
    "epidenet_1ch_og.add(tf.keras.layers.InputLayer((INPUT_WINDOW_SIZE, INPUT_NB_CHANNELS, 1)))\n",
    "epidenet_1ch_og.add(tf.keras.layers.Conv2D(4, kernel_size=(4, 1), padding=\"same\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_og.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.MaxPooling2D(pool_size=(8, 1)))\n",
    "epidenet_1ch_og.add(tf.keras.layers.Conv2D(16, kernel_size=(16, 1), padding=\"same\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_og.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 1)))\n",
    "epidenet_1ch_og.add(tf.keras.layers.Conv2D(16, kernel_size=(8, 1), padding=\"same\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_og.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_og.add(tf.keras.layers.AveragePooling2D(pool_size=(8, 1)))\n",
    "epidenet_1ch_og.add(tf.keras.layers.Flatten())\n",
    "epidenet_1ch_og.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "# epidenet_1ch_og.summary()\n",
    "\n",
    "# variant 3.2\n",
    "epidenet_1ch_v3_2 = tf.keras.Sequential()\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.InputLayer((INPUT_WINDOW_SIZE, INPUT_NB_CHANNELS, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(4, kernel_size=(4, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.MaxPooling2D(pool_size=(8, 1)))\n",
    "\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(16, kernel_size=(5, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(16, kernel_size=(5, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(16, kernel_size=(5, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(16, kernel_size=(4, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 1)))\n",
    "\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.ZeroPadding2D(padding=(2, 0)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Conv2D(16, kernel_size=(8, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.BatchNormalization())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Activation(\"relu\"))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 1)))\n",
    "\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.AveragePooling2D(pool_size=(7, 1), strides=(1, 1)))\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Flatten())\n",
    "epidenet_1ch_v3_2.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "epidenet_1ch_v3_2.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d_18 (ZeroPad  (None, 1020, 1, 1)        0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 1017, 1, 4)        20        \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 1017, 1, 4)        16        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 1017, 1, 4)        0         \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPooli  (None, 127, 1, 4)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " zero_padding2d_19 (ZeroPad  (None, 131, 1, 4)         0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 127, 1, 16)        336       \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 127, 1, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 127, 1, 16)        0         \n",
      "                                                                 \n",
      " zero_padding2d_20 (ZeroPad  (None, 131, 1, 16)        0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 127, 1, 16)        1296      \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 127, 1, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 127, 1, 16)        0         \n",
      "                                                                 \n",
      " zero_padding2d_21 (ZeroPad  (None, 131, 1, 16)        0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 127, 1, 16)        1296      \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 127, 1, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 127, 1, 16)        0         \n",
      "                                                                 \n",
      " zero_padding2d_22 (ZeroPad  (None, 131, 1, 16)        0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 128, 1, 16)        1040      \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 128, 1, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 128, 1, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 32, 1, 16)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " zero_padding2d_23 (ZeroPad  (None, 36, 1, 16)         0         \n",
      " ding2D)                                                         \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 29, 1, 16)         2064      \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 29, 1, 16)         64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 29, 1, 16)         0         \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 7, 1, 16)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " average_pooling2d_9 (Avera  (None, 1, 1, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6405 (25.02 KB)\n",
      "Trainable params: 6237 (24.36 KB)\n",
      "Non-trainable params: 168 (672.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load train and test datasets",
   "id": "1c16213a040c21a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:00:15.005370Z",
     "start_time": "2024-09-02T15:00:14.772073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from brainmepnas import Dataset\n",
    "\n",
    "dataset = Dataset(\"data/chbmit_singlech\")\n",
    "train_data = dataset.get_data({\"5\": [1, 2, 3, 4]}, set=\"train\", shuffle=True, shuffle_seed=42)\n",
    "test_data = dataset.get_data({\"5\": [0]}, set=\"test\", shuffle=False)"
   ],
   "id": "625b778585b92367",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train models",
   "id": "6138107ce295da40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:03:41.323132Z",
     "start_time": "2024-09-02T15:01:29.363636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "monitoring_metrics = [tf.keras.metrics.AUC(num_thresholds=25,\n",
    "                                           curve='PR',\n",
    "                                           name=\"auc_pr\")]\n",
    "\n",
    "epidenet_1ch_og.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1**-4,\n",
    "                                beta_1=0.9, beta_2=0.999), \n",
    "                        loss=\"binary_crossentropy\",\n",
    "              metrics=monitoring_metrics)\n",
    "epidenet_1ch_v3_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1**-4,\n",
    "                                beta_1=0.9, beta_2=0.999), \n",
    "                          loss=\"binary_crossentropy\",\n",
    "              metrics=monitoring_metrics)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=10,\n",
    "                                                      mode=\"min\",\n",
    "                                                      start_from_epoch=10)]\n",
    "\n",
    "# Train model\n",
    "epidenet_1ch_og.fit(train_data[0][:,:1016], train_data[1],\n",
    "                    validation_split=0.2,\n",
    "                    epochs=1000, batch_size=256,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    use_multiprocessing=False, shuffle=False)\n",
    "epidenet_1ch_v3_2.fit(train_data[0][:,:1016], train_data[1],\n",
    "                    validation_split=0.2,\n",
    "                    epochs=1000, batch_size=256,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    use_multiprocessing=False, shuffle=False)"
   ],
   "id": "d2c193d286629d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "45/45 [==============================] - 3s 46ms/step - loss: 0.0735 - auc_pr: 0.7676 - val_loss: 8.5859 - val_auc_pr: 0.0194\n",
      "Epoch 2/1000\n",
      "45/45 [==============================] - 2s 46ms/step - loss: 0.0270 - auc_pr: 0.9139 - val_loss: 0.0886 - val_auc_pr: 0.7748\n",
      "Epoch 3/1000\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.0213 - auc_pr: 0.9373 - val_loss: 0.0419 - val_auc_pr: 0.8348\n",
      "Epoch 4/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0217 - auc_pr: 0.9403 - val_loss: 0.1109 - val_auc_pr: 0.6852\n",
      "Epoch 5/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0214 - auc_pr: 0.9400 - val_loss: 0.0775 - val_auc_pr: 0.7053\n",
      "Epoch 6/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0225 - auc_pr: 0.9390 - val_loss: 0.6053 - val_auc_pr: 0.3423\n",
      "Epoch 7/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0193 - auc_pr: 0.9476 - val_loss: 0.0904 - val_auc_pr: 0.7810\n",
      "Epoch 8/1000\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.0276 - auc_pr: 0.9127 - val_loss: 1.5369 - val_auc_pr: 0.1199\n",
      "Epoch 9/1000\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.0254 - auc_pr: 0.9268 - val_loss: 0.0570 - val_auc_pr: 0.8293\n",
      "Epoch 10/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0261 - auc_pr: 0.9292 - val_loss: 0.0673 - val_auc_pr: 0.8097\n",
      "Epoch 11/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0182 - auc_pr: 0.9505 - val_loss: 0.0572 - val_auc_pr: 0.7955\n",
      "Epoch 12/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0149 - auc_pr: 0.9621 - val_loss: 0.0742 - val_auc_pr: 0.7510\n",
      "Epoch 13/1000\n",
      "45/45 [==============================] - 2s 38ms/step - loss: 0.0140 - auc_pr: 0.9659 - val_loss: 0.0612 - val_auc_pr: 0.8080\n",
      "Epoch 14/1000\n",
      "45/45 [==============================] - 2s 42ms/step - loss: 0.0368 - auc_pr: 0.8658 - val_loss: 2.4022 - val_auc_pr: 0.0401\n",
      "Epoch 15/1000\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.0216 - auc_pr: 0.9425 - val_loss: 0.0467 - val_auc_pr: 0.8291\n",
      "Epoch 16/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0157 - auc_pr: 0.9606 - val_loss: 0.0517 - val_auc_pr: 0.8432\n",
      "Epoch 17/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0152 - auc_pr: 0.9577 - val_loss: 0.0510 - val_auc_pr: 0.8508\n",
      "Epoch 18/1000\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 0.0162 - auc_pr: 0.9596 - val_loss: 0.0840 - val_auc_pr: 0.8255\n",
      "Epoch 19/1000\n",
      "45/45 [==============================] - 2s 42ms/step - loss: 0.0204 - auc_pr: 0.9442 - val_loss: 0.1422 - val_auc_pr: 0.6990\n",
      "Epoch 20/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0183 - auc_pr: 0.9567 - val_loss: 0.0510 - val_auc_pr: 0.8539\n",
      "Epoch 21/1000\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 0.0212 - auc_pr: 0.9446 - val_loss: 0.1047 - val_auc_pr: 0.7360\n",
      "Epoch 22/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0151 - auc_pr: 0.9639 - val_loss: 0.0699 - val_auc_pr: 0.8549\n",
      "Epoch 23/1000\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 0.0177 - auc_pr: 0.9521 - val_loss: 0.0672 - val_auc_pr: 0.8240\n",
      "Epoch 24/1000\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 0.0149 - auc_pr: 0.9648 - val_loss: 0.0544 - val_auc_pr: 0.7968\n",
      "Epoch 25/1000\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.0232 - auc_pr: 0.9276 - val_loss: 0.1311 - val_auc_pr: 0.6796\n",
      "Epoch 1/1000\n",
      "45/45 [==============================] - 4s 74ms/step - loss: 0.2019 - auc_pr: 0.2877 - val_loss: 1.8744 - val_auc_pr: 0.0476\n",
      "Epoch 2/1000\n",
      "45/45 [==============================] - 3s 76ms/step - loss: 0.0590 - auc_pr: 0.7328 - val_loss: 0.3419 - val_auc_pr: 0.1338\n",
      "Epoch 3/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0622 - auc_pr: 0.7125 - val_loss: 2.8819 - val_auc_pr: 0.0165\n",
      "Epoch 4/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0378 - auc_pr: 0.8533 - val_loss: 3.4401 - val_auc_pr: 0.0203\n",
      "Epoch 5/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0303 - auc_pr: 0.8913 - val_loss: 4.3922 - val_auc_pr: 0.0387\n",
      "Epoch 6/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0296 - auc_pr: 0.8914 - val_loss: 0.3780 - val_auc_pr: 0.3256\n",
      "Epoch 7/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0472 - auc_pr: 0.7944 - val_loss: 3.3363 - val_auc_pr: 0.0457\n",
      "Epoch 8/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0375 - auc_pr: 0.8635 - val_loss: 0.0496 - val_auc_pr: 0.7922\n",
      "Epoch 9/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0303 - auc_pr: 0.8901 - val_loss: 0.0803 - val_auc_pr: 0.5841\n",
      "Epoch 10/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0266 - auc_pr: 0.9129 - val_loss: 0.0815 - val_auc_pr: 0.6647\n",
      "Epoch 11/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0290 - auc_pr: 0.8984 - val_loss: 0.0414 - val_auc_pr: 0.8475\n",
      "Epoch 12/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0321 - auc_pr: 0.8802 - val_loss: 0.0508 - val_auc_pr: 0.8341\n",
      "Epoch 13/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0312 - auc_pr: 0.8826 - val_loss: 0.0305 - val_auc_pr: 0.8780\n",
      "Epoch 14/1000\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0302 - auc_pr: 0.8921 - val_loss: 0.6263 - val_auc_pr: 0.2653\n",
      "Epoch 15/1000\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.0372 - auc_pr: 0.8393 - val_loss: 0.0431 - val_auc_pr: 0.8361\n",
      "Epoch 16/1000\n",
      "45/45 [==============================] - 3s 70ms/step - loss: 0.0265 - auc_pr: 0.9039 - val_loss: 0.0265 - val_auc_pr: 0.9307\n",
      "Epoch 17/1000\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0273 - auc_pr: 0.8998 - val_loss: 0.1950 - val_auc_pr: 0.6003\n",
      "Epoch 18/1000\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.0334 - auc_pr: 0.8676 - val_loss: 0.0221 - val_auc_pr: 0.9306\n",
      "Epoch 19/1000\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0247 - auc_pr: 0.9121 - val_loss: 0.1356 - val_auc_pr: 0.8562\n",
      "Epoch 20/1000\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0247 - auc_pr: 0.9198 - val_loss: 0.3947 - val_auc_pr: 0.4935\n",
      "Epoch 21/1000\n",
      "45/45 [==============================] - 3s 66ms/step - loss: 0.0218 - auc_pr: 0.9258 - val_loss: 0.1887 - val_auc_pr: 0.7323\n",
      "Epoch 22/1000\n",
      "45/45 [==============================] - 3s 70ms/step - loss: 0.0246 - auc_pr: 0.9129 - val_loss: 2.4957 - val_auc_pr: 0.0756\n",
      "Epoch 23/1000\n",
      "45/45 [==============================] - 4s 89ms/step - loss: 0.0266 - auc_pr: 0.9093 - val_loss: 0.4639 - val_auc_pr: 0.3454\n",
      "Epoch 24/1000\n",
      "45/45 [==============================] - 3s 76ms/step - loss: 0.0278 - auc_pr: 0.8904 - val_loss: 0.0356 - val_auc_pr: 0.8694\n",
      "Epoch 25/1000\n",
      "45/45 [==============================] - 3s 75ms/step - loss: 0.0238 - auc_pr: 0.9196 - val_loss: 0.0308 - val_auc_pr: 0.8904\n",
      "Epoch 26/1000\n",
      "45/45 [==============================] - 4s 81ms/step - loss: 0.0229 - auc_pr: 0.9130 - val_loss: 0.0424 - val_auc_pr: 0.8961\n",
      "Epoch 27/1000\n",
      "45/45 [==============================] - 3s 67ms/step - loss: 0.0245 - auc_pr: 0.9119 - val_loss: 0.0476 - val_auc_pr: 0.8906\n",
      "Epoch 28/1000\n",
      "45/45 [==============================] - 3s 67ms/step - loss: 0.0240 - auc_pr: 0.9132 - val_loss: 0.2075 - val_auc_pr: 0.5842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fed505dacd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test model on test set",
   "id": "e0815e5f680a571e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:03:59.245123Z",
     "start_time": "2024-09-02T15:03:56.139835Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4a840bbb293444a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 5ms/step\n",
      "225/225 [==============================] - 2s 6ms/step\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We convert the models to tflite.",
   "id": "77b549a8b43a88dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:05:46.970338Z",
     "start_time": "2024-09-02T15:05:46.954150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# import built-in module\n",
    "import tempfile\n",
    "from typing import Literal, Optional\n",
    "\n",
    "# import third-party modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import your own module\n",
    "\n",
    "\n",
    "def generate_tflite_model(keras_model: tf.keras.Model,\n",
    "                          input_format: Literal[\"float\", \"int8\"],\n",
    "                          output_format: Literal[\"float\", \"int8\"],\n",
    "                          representative_input: Optional[np.ndarray] = None):\n",
    "    \"\"\"\n",
    "    Convert the given keras model to a tflite model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    keras_model: tf.keras.Model\n",
    "        Keras model to convert.\n",
    "    input_format: Literal[\"float\", \"int8\"]\n",
    "        Format of the input to the keras model.\n",
    "    output_format: Literal[\"float\", \"int8\"]\n",
    "        Format of the output of the keras model.\n",
    "    representative_input: np.ndarray, optional\n",
    "        Representative input data, used to perform the quantization. If not\n",
    "        given, quantization is performed using randomly generated data between\n",
    "        -1 and 1 (float) or -128 and 127 (int8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tflite_model: tf.keras.Model\n",
    "    \"\"\"\n",
    "    if input_format == \"float\":\n",
    "        input_tensor_type = tf.float32\n",
    "        representative_dataset_min = -1\n",
    "        representative_dataset_max = 1\n",
    "    elif input_format == \"int8\":\n",
    "        input_tensor_type = tf.int8\n",
    "        representative_dataset_min = -128\n",
    "        representative_dataset_max = 127\n",
    "    else:\n",
    "        raise ValueError(f\"input_format={input_format} is not supported.\")\n",
    "\n",
    "    if output_format == \"float\":\n",
    "        output_tensor_type = tf.float32\n",
    "    elif output_format == \"int8\":\n",
    "        output_tensor_type = tf.int8\n",
    "    else:\n",
    "        raise ValueError(f\"output_format={output_format} is not supported.\")\n",
    "\n",
    "    keras_model.build()\n",
    "\n",
    "    if representative_input is None:\n",
    "        def representative_dataset():\n",
    "            nb_samples = 100\n",
    "            min_value = representative_dataset_min\n",
    "            max_value = representative_dataset_max\n",
    "            input_shape = keras_model.input_shape[1:]\n",
    "            for i in range(nb_samples):\n",
    "                data = (max_value - min_value) * np.random.random_sample(input_shape) + min_value\n",
    "                data = data.astype(np.float32)\n",
    "                yield [np.expand_dims(data, axis=0)]\n",
    "    else:\n",
    "        def representative_dataset():\n",
    "            for x in representative_input:\n",
    "                yield [np.expand_dims(x, axis=[0, 3]).astype(\n",
    "                np.float32)]\n",
    "\n",
    "    # Bug: tensorflow 2.16.1\n",
    "    # converter.convert() raises AttributeError, see https://github.com/tensorflow/tensorflow/issues/63867\n",
    "    # Fix is to use save model.\n",
    "    # converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        keras_model.export(tmp_dir)\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tmp_dir)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Should always be set to [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # Should always be set to [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = input_tensor_type  # Either tf.float32, tf.int8 (recommended), tf.uint8\n",
    "        converter.inference_output_type = output_tensor_type  # Either tf.float32, tf.int8 (recommended), tf.uint8\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "    return tflite_model"
   ],
   "id": "fb58f3e0b49602a8",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:26:22.075228Z",
     "start_time": "2024-09-02T15:26:16.599432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epidenet_1ch_og_tflite = generate_tflite_model(epidenet_1ch_og, input_format=\"float\", output_format=\"float\", representative_input=train_data[0][:,:1016])\n",
    "with open(\"epidenet_1ch_og.tflite\", \"wb\") as f:\n",
    "    f.write(epidenet_1ch_og_tflite)\n",
    "    \n",
    "epidenet_1ch_v3_2_tflite = generate_tflite_model(epidenet_1ch_v3_2, input_format=\"float\", output_format=\"float\", representative_input=train_data[0][:,:1016])\n",
    "with open(\"epidenet_1ch_v3_2.tflite\", \"wb\") as f:\n",
    "    f.write(epidenet_1ch_v3_2_tflite)"
   ],
   "id": "4b112c8787164a00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpd7dh4qao/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpd7dh4qao/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpd7dh4qao'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  Args:\n",
      "    args_0: float32 Tensor, shape=(None, 1016, 1, 1)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(None, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 17:26:16.969913: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-09-02 17:26:16.969959: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-09-02 17:26:16.970092: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpd7dh4qao\n",
      "2024-09-02 17:26:16.970499: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-09-02 17:26:16.970505: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpd7dh4qao\n",
      "2024-09-02 17:26:16.971603: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-09-02 17:26:16.993043: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpd7dh4qao\n",
      "2024-09-02 17:26:17.001339: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 31246 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuiltd5da/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuiltd5da/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpuiltd5da'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  Args:\n",
      "    args_0: float32 Tensor, shape=(None, 1016, 1, 1)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(None, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 17:26:18.867921: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-09-02 17:26:18.867978: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-09-02 17:26:18.868116: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpuiltd5da\n",
      "2024-09-02 17:26:18.868660: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-09-02 17:26:18.868668: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpuiltd5da\n",
      "2024-09-02 17:26:18.870370: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-09-02 17:26:18.901595: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpuiltd5da\n",
      "2024-09-02 17:26:18.912237: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 44120 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test quantized model",
   "id": "ad62057c1b3fe435"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:26:22.983773Z",
     "start_time": "2024-09-02T15:26:22.476226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from brainmepnas import AccuracyMetrics\n",
    "import csv\n",
    "\n",
    "\n",
    "interpreter_og = tf.lite.Interpreter(\"epidenet_1ch_og.tflite\")\n",
    "interpreter_og.allocate_tensors()\n",
    "input_details = interpreter_og.get_input_details()\n",
    "output_details = interpreter_og.get_output_details()\n",
    "predicted_og = []\n",
    "formatted_test_data = np.expand_dims(test_data[0][:,:1016].astype(np.float32), axis=3)\n",
    "for sample in formatted_test_data:\n",
    "    interpreter_og.set_tensor(input_details[0][\"index\"], [sample])\n",
    "    interpreter_og.invoke()\n",
    "    predicted_og.append(interpreter_og.get_tensor(output_details[0][\"index\"]))\n",
    "\n",
    "am_og = AccuracyMetrics(test_data[1].flatten(), np.array(predicted_og).flatten(), sample_duration=1016/256, sample_offset=2, threshold=\"max_f_score\")\n",
    "\n",
    "interpreter_v3_2 = tf.lite.Interpreter(\"epidenet_1ch_v3_2.tflite\")\n",
    "interpreter_v3_2.allocate_tensors()\n",
    "input_details = interpreter_v3_2.get_input_details()\n",
    "output_details = interpreter_v3_2.get_output_details()\n",
    "predicted_v3_2 = []\n",
    "formatted_test_data = np.expand_dims(test_data[0][:,:1016].astype(np.float32), axis=3)\n",
    "for sample in formatted_test_data:\n",
    "    interpreter_v3_2.set_tensor(input_details[0][\"index\"], [sample])\n",
    "    interpreter_v3_2.invoke()\n",
    "    predicted_v3_2.append(interpreter_v3_2.get_tensor(output_details[0][\"index\"]))\n",
    "\n",
    "am_v3_2 = AccuracyMetrics(test_data[1].flatten(), np.array(predicted_v3_2).flatten(), sample_duration=1016/256, sample_offset=2, threshold=\"max_f_score\")\n",
    "\n",
    "with open(\"epidenet_1ch_og.csv\", \"w\") as f:\n",
    "    am_dict = am_og.as_dict()\n",
    "    writer = csv.DictWriter(f, fieldnames=am_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(am_dict)\n",
    "    \n",
    "with open(\"epidenet_1ch_v3_2.csv\", \"w\") as f:\n",
    "    am_dict = am_v3_2.as_dict()\n",
    "    writer = csv.DictWriter(f, fieldnames=am_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(am_dict)"
   ],
   "id": "37c1fba894b4320f",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cbd29485d864b795"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
